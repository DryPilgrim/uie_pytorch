import sys
import os
import random
import json
import shutil
import re
import heapq
import numpy as np
import pandas as pd
from pandas import DataFrame
import lightgbm as lgb
from termcolor import colored
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple
from quark_argument_parser import QuarkArgumentParser
from quark_logger import get_logger
from utils.logger_utils import init_logger
from lgb.lgb_score import calc_score_inv, ensemble_eval, ensemble_predict
from lgb.lgb_engine import cv


@dataclass
class DataArguments:
    name = 'data'
    train_data_path: str = field(
        metadata={"help": "train.csv with `label` field"})
    val_data_path: Optional[str] = field(
        default=None,
        metadata={"help": "eval.csv with `label` field"})
    test_data_path: Optional[str] = field(
        default=None,
        metadata={"help": "test.csv with `label` field"})
    test_split_ratio: Optional[float] = field(
        default=0.2,
        metadata={"help": "ratio: val/data"})
    test_split_seed: Optional[int] = field(
        default=88,
        metadata={"help": "seed: 88"})
    test_split_mode: Optional[str] = field(
        default='random',
        metadata={"help": "split mode: `custom` or `random`"})


@dataclass
class TrainingArguments:
    name = 'train'
    lgbm_params: dict = field(
        metadata={"help": "lgbm config path"})
    lgbm_type: str = field(
        metadata={"help": "lgbm type in `regression` or `cv`"})
    lgbm_num_boost_round: int = field(
        metadata={"help": "lgbm_num_boost_round"})
    lgbm_forced_num_boost_round: Optional[int] = field(
        default=None,
        metadata={"help": "lgbm_forced_num_boost_round"})
    lgbm_forced_enable: Optional[bool] = field(
        default=False,
        metadata={"help": "lgbm train for forced or not"})
    save_model_path: str = field(
        default='model',
        metadata={"help": "save model dir path"})
    label_mapping_enable: Optional[bool] = field(
        default=False,
        metadata={"help": "mapping label to Integer"})
    score_inv_gap: Optional[int] = field(
        default=0,
        metadata={"help": "score_inv_gap on test dataset"})

# 子类需实现的方法：
# @required dataframe_preprocess(df) -> train_df, feature_fields, categorical_feature
# @optional dataframe_split_for_train(df) -> train_folds, val_folds
# @optional dataframe_split_for_test(df) -> train_df, test_df
# @optional handle_predict_result(df)
class LGBModel:
    def __init__(self, job_name, json_file_path):
        self._job_name = job_name
        global logger
        logger = init_logger(get_logger(job_name))
        self._logger = logger

        logger.info(colored(f'==> LGBModel初始化开始,读取{json_file_path}', 'yellow'))

        parser = QuarkArgumentParser([DataArguments, TrainingArguments])
        data_args, training_args = parser.parse_json_file(json_file=json_file_path)

        logger.info(colored(f'==> LGBModel参数解析完成', 'yellow'))
        logger.info(colored(f'====> Data参数: {json.dumps(data_args, ensure_ascii=False, default=lambda obj: obj.__dict__)}', 'blue'))
        logger.info(colored(f'====> Training参数: {json.dumps(training_args, ensure_ascii=False, default=lambda obj: obj.__dict__)}', 'blue'))

        self._data_args = data_args
        self._training_args = training_args
        self._train_df = None
        self._feature_fields = None
        self._categorical_feature = None

    def set_ext_param(self, ext_param):
        self._ext_param = ext_param

    def train(self):
        logger.info(colored(f'==> 模型训练开始', 'yellow'))
        logger.info(colored(f'==> 数据准备开始', 'yellow'))

        # step1. 备份数据csv文件和相关配置
        if os.path.exists(self._training_args.save_model_path):
            shutil.rmtree(self._training_args.save_model_path)
        os.makedirs(self._training_args.save_model_path + '/data')
        shutil.copyfile(os.path.abspath(self._job_name + '.json'), self._training_args.save_model_path + '/data/' + self._job_name + '.json')

        train_dataset, train_folds, val_folds = self._load_train_dataset()
        # print(len(train_folds), len(val_folds))
        logger.info(colored(f'==> 数据加载完成', 'yellow'))

        # step2. base模型训练
        baseModel_params = self._training_args.lgbm_params
        logger.info(colored(f'==> base模型开始训练', 'yellow'))

        resultModel = lgb.cv(
            baseModel_params,
            train_dataset,
            num_boost_round=self._training_args.lgbm_num_boost_round,
            folds=zip(train_folds, val_folds),
            shuffle=False,
            callbacks=[lgb.early_stopping(20), lgb.log_evaluation(50)],
            return_cvbooster=True
            )
        logger.info(colored(f'==> base模型训练结束', 'yellow'))

        # step3. forced模型是否进行训练
        if self._training_args.lgbm_forced_enable:
            resultModel['cvbooster'].free_dataset()

            logger.info(colored(f'==> forced模型开始训练', 'yellow'))
            baseModel_params['feature_fraction'] = 1
            resultModel = cv(
                baseModel_params,
                train_dataset,
                num_boost_round=self._training_args.lgbm_forced_num_boost_round,
                folds=zip(train_folds, val_folds),
                shuffle=False,
                init_model=resultModel['cvbooster'].boosters,
                callbacks=[lgb.log_evaluation(50), ],
                return_cvbooster=True
            )
            logger.info(colored(f'==> forced模型训练结束', 'yellow'))
        self._model_save(models=resultModel['cvbooster'].boosters)

        # step4. 模型对test进行评估
        logger.info(colored(f"==> 模型开始评估", 'yellow'))
        test_dataset, _, _ = self._load_test_dataset()
        test_dataset.set_reference(train_dataset)

        eval_result = ensemble_eval(
            models_list=resultModel['cvbooster'].boosters,
            test_dataset=test_dataset
        )
        eval_result_print = json.dumps(eval_result, indent=1, separators=(', ', ': '), ensure_ascii=False)
        logger.info(colored(f"==> 各个模型对测试集的评估如下{eval_result_print}", 'blue'))

        if self._training_args.lgbm_forced_enable:
            resultModel['cvbooster'].free_dataset()

    def test(self, models_path=None):
        logger.info(colored(f"==> 模型开始测试", 'yellow'))
        # step1. load models
        if models_path is None:
            models_path = [os.path.join(self._training_args.save_model_path, path) 
                            for path in os.listdir(self._training_args.save_model_path)
                            if path.endswith('.txt')]
        if not isinstance(models_path, list):
            raise TypeError(f'models_path must be list, but get {type(models_path)} instead')
        logger.info(colored(f'==> 开始加载模型，模型总数为{len(models_path)}， 获取路径如下{models_path}', 'yellow'))
        models_list = self._load_models_from_path(load_path=models_path)
        logger.info(colored(f'==> 模型加载完毕', 'yellow'))

        # step2. load test data and train data
        logger.info(colored(f"==> 测试数据集开始加载", 'yellow'))
        test_dataset, data_x, data_y = self._load_test_dataset()
        logger.info(colored(f"==> 测试数据集加载完毕", 'yellow'))

        # step3. eval test data
        logger.info(colored("==> 模型对测试集进行预测并评分", 'yellow'))
        logger.info(colored("====> ！！注意：单个预测结果为多个模型预测后的加和均值", 'red'))
        data_y_pred = ensemble_predict(models_list, data_x)
        inv_score = calc_score_inv(data_y, data_y_pred, self._training_args.score_inv_gap)
        logger.info(colored(f"====> 模型在测试集上的正逆序得分为{inv_score}", 'yellow'))

        if self.handle_predict_result:
            predict_result_df = DataFrame({'label': data_y, 'label_pred': data_y_pred}, index = list(data_x.index.values))
            self.handle_predict_result(predict_result_df)

    def _model_save(self, models):
        assert models is not None and type(models) == list, 'please pass correct models'
        logger.info(colored(f'==> 模型开始保存', 'yellow'))
        logger.info(colored(f'====> 模型以及模型特征使用频率保存至{self._training_args.save_model_path}目录下', 'green'))
        logger.info(colored('====> ！！注意：3.3.3及以下版本的lgbm不支持CV模型的一次性保存', 'red'))
        df_importance = pd.DataFrame(index=models[0].feature_name())
        for n, model in enumerate(models):
            model.save_model(os.path.join(self._training_args.save_model_path, f'model_{n}_th.txt'))
            df_importance[f'{n}th_model'] = model.feature_importance()
        df_importance['mean_importance'] = df_importance.mean(axis=1)
        df_importance.sort_values(by=['mean_importance'], ascending=False, inplace=True)
        df_importance.to_csv(os.path.join(self._training_args.save_model_path, 'model_importance.csv'))
        logger.info(colored(f'====> ！！注意：1. 特征重要度按照模型划分节点时对特征的引用次数进行排序；2. 输出的模型特征重要度按照不同模型的均值排序。要查看更详细的排序结果请移步csv文件', 'red'))
        logger.info(colored(f"====> 重要度前10特征如下: \n{df_importance.iloc[:10, -1]}", 'green'))
        logger.info(colored(f"==> 模型保存完成", 'yellow'))

    def _load_models_from_path(self, load_path):
        models = []
        for num in range(len(load_path)):
            models.append(lgb.Booster(model_file=load_path[num]))
        return models

    def _load_train_dataset(self):
        data_args, training_args = self._data_args, self._training_args

        # step1. train_df load & preprocess
        if self._train_df is None or self._feature_fields is None:
            if not os.path.exists(data_args.train_data_path):
                raise ValueError(f"train.csv `{data_args.train_data_path}` not exists.")
            train_df = pd.read_csv(data_args.train_data_path)
            train_df, feature_fields, categorical_feature = self.dataframe_preprocess(train_df)
            train_df = self._label_mapping(train_df)
            self._train_df = train_df
            self._feature_fields = feature_fields
            self._categorical_feature = categorical_feature
            logger.info(colored(f'==> 加载训练集概要统计: \n{train_df.describe()}', 'blue'))

        # step2. split train_df -> train_folds, val_folds
        train_folds, val_folds = self.dataframe_split_for_train(self._train_df)

        # step3. train_df to lgbm dataset
        ydata_df = self._train_df['label']
        xdata_df = self._train_df[self._feature_fields]
        train_dataset = lgb.Dataset(
            data=xdata_df,
            label=ydata_df,
            feature_name=self._feature_fields,
            categorical_feature=categorical_feature,
            free_raw_data=False
        )
        return train_dataset, train_folds, val_folds
    
    def _load_test_dataset(self):
        data_args, traing_args = self._data_args, self._training_args
        # step1. load test test_df
        if data_args.test_data_path is not None:
            if not os.path.exists(data_args.test_data_path):
                raise ValueError(f"train.csv `{data_args.test_data_path}` not exists.")
            logger.info(colored(f'====> 测试数据从`{data_args.test_data_path}`中采样', 'yellow'))
            test_df = pd.read_csv(data_args.test_data_path)
            test_df, feature_fields, categorical_feature = self.dataframe_preprocess(test_df)
            test_df = self._label_mapping(test_df)
        elif data_args.test_split_mode == 'random' or data_args.test_split_mode == 'custom':
            logger.info(colored(f'====> 测试数据从原始数据中采样', 'yellow'))
            if self._train_df is None or self._feature_fields is None:
                train_df = pd.read_csv(data_args.train_data_path)
                train_df, feature_fields, categorical_feature = self.dataframe_preprocess(train_df)
                train_df = self._label_mapping(train_df)
                self._train_df = train_df
                self._feature_fields = feature_fields
                self._categorical_feature = categorical_feature
            feature_fields = self._feature_fields
            categorical_feature = self._categorical_feature

            if data_args.test_split_mode == 'random':
                test_df = self._train_df.sample(frac=data_args.test_split_ratio, random_state=data_args.test_split_seed)
            elif data_args.test_split_mode == 'custom':
                test_df = self.dataframe_split_for_test(self._train_df)

        # step2. test_train_df -> dataset
        test_dataset = lgb.Dataset(
            data=test_df[feature_fields],
            label=test_df['label'],
            feature_name=feature_fields,
            categorical_feature=categorical_feature,
            free_raw_data=True
        )
        return test_dataset, test_df[feature_fields], test_df['label']

    def _save_label_mapping(self, mapping_label_dict):
        path = self._training_args.save_model_path + '/label_mapping.json'
        with open(path, 'w') as label_mapping_file:
            json.dump(mapping_label_dict, label_mapping_file)

    def _read_label_mapping(self):
        path = self._training_args.save_model_path + '/label_mapping.json'
        if not os.path.exists(path):
            return None
        with open(path, 'r') as label_mapping_file:
            return json.load(label_mapping_file)

    def _label_mapping(self, data_df):
        if not self._training_args.label_mapping_enable:
            return data_df

        ori_labels = []
        mapping_labels = []
        mapping_label_dict = {}
        for mapping_label, ori_label in enumerate(data_df['label'].value_counts().keys()):
            mapping_labels.append(mapping_label)
            ori_labels.append(ori_label)
            mapping_label_dict[ori_label] = mapping_label
        if len(set(ori_labels) & set(mapping_labels)) != len(ori_labels):
            self._save_label_mapping(mapping_label_dict)
        else:
            mapping_label_dict = None

        label_class = len(data_df['label'].value_counts().keys().tolist())
        if mapping_label_dict is not None:
            data_df['label'] = data_df['label'].map(lambda x: mapping_label_dict[x])
        return data_df
